\documentclass{article}

\newcommand{\rank}{\textrm{rank}}
\newcommand{\tr}{\textrm{tr}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\Y}{\mathbb{Y}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\argmin}{\mathop{\rm arg\,min}}
\newcommand{\ev}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\prob}[1]{\mathbb{P}\left\{#1\right\}}
\newcommand{\var}{\textrm{var}}
\newcommand{\subG}{\mathsf{subG}}
\newcommand{\iid}{\overset{\textrm{iid}}{\sim}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\eps}{\varepsilon} % best epsilon
\usepackage{amssymb, amsmath, amsthm}
\usepackage{enumerate}

\usepackage{palatino}



\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

\title{Least squares with sparsity and generalizations}
\author{Notes by Parker Knight}


\begin{document}
\maketitle

%% \begin{abstract}
%% 	It is well known that, in the low-dimensional setting, the OLS estimator
%% 	enjoys several nice properties, including consistency. However, in the
%% 	high-dimensional regime, the aspect ratio stays bounded away from 0, and
%% 	the estimator may not converge to the true parameter value. We may
%% 	ameliorate this shortcoming by imposing additional structure on the true
%% 	parameter; in particular, we assume that the parameter vector is sparse.
%%
%% 	This note set will study nonasymptotic bounds on estimation and
%% 	prediction error for the LASSO, the most well-known variant of sparse regression. To do so, we first develop a
%% 	theory of tail bounds and sub-Gaussian random variables. After studying
%% 	the LASSO, we give a brief treatment of general regularized M-estimators.
%% \end{abstract}

\section{Preliminaries}

\subsection{Basic tail bounds}

The development of a nonasymptotic theory involves understanding the extreme
behavior of random variables; more specifically, we seek to study how random
variables fluctuate around their mean. Our primary tool to do so is known as
\textit{Markov's inequality}, which is stated and proven below.

\begin{theorem}[Markov's inequality]
	Let $X$ be a random variable and let $g(.)$ be a
	nonnegative function. Then for $t \geq 0$

	$$\prob{g(X) \geq t} \leq \frac{\ev{g(X)}}{t}$$
\end{theorem}

\begin{proof}
	\begin{align*}
		\ev{g(X)} &= \int_{\R}g(x)f(x)dx \\ 
		&\geq \int_{\{x: g(x) \geq t\}}g(x)f(x)dx \quad \textrm{(using nonnegativity of $g$)} \\
		&\geq t\int_{\{x : g(x) \geq t}f(x)dx \\
		&= t\prob{g(X) \geq t}
	\end{align*}

	Rearranging terms gives the result.
\end{proof}

Through a careful choice of function $g(.)$, we can control the tails of $X$
rather elegantly.

\begin{corollary}[Chebyshev's inequality]
	Let $X$ be a random variable with a finite second moment. Then for $t
	\geq 0$

	$$\prob{|X - \ev{X}| \geq t} \leq \frac{\var(X)}{t^2}$$
\end{corollary}

\begin{proof}
	By direct calculation:

	\begin{align*}
		\prob{|X - \ev{X}| \geq t} &= \prob{(X - \ev{X})^2 \geq t^2} \\ 
		&\leq \frac{\ev{(X  - \ev{X})^2}}{t^2} \quad \textrm{(by Markov)} \\
		&= \frac{\var(X)}{t^2}
	\end{align*}
\end{proof}

Chebyshev's inequality requires only the existence of a second moment, but in
many cases, can be quite loose. We can obtain tighter bounds under more
stringent conditions on $X$. 

\begin{corollary}[Chernoff bound]
	Let $X$ be a random variable with a moment-generating function that
	exists at all $\lambda \in \R$.  Then for $t \geq 0$

	$$\prob{X \geq t} \leq \inf_{\lambda \in \R}e^{-t\lambda}\ev{e^{\lambda X}}$$
\end{corollary}

\begin{proof}
	Apply Markov with the function $g(x) = e^{\lambda x}$.
\end{proof}

The Chernoff bound allows us to control the tails of $X$ with its moment
generating function. Often, this can give us much tighter bounds than those
obtained by Chebyshev. 

For example, let $X \sim N(0, \sigma^2)$. A simple calculation yields
$\ev{e^{\lambda X}} = e^{\sigma^2 \lambda^2 / 2}$ for all $\lambda \in \R$. The
Chernoff bound yields 

$$\prob{X \geq t} \leq \inf_{\lambda \in \R}e^{\sigma^2\lambda^2/2 - \lambda
t}$$

Some calculus reveals that this infimum is attained at $\lambda = t / \sigma^2$,
yielding an upper bound of 

$$\prob{X \geq t} \leq \exp\left[-\frac{t^2}{2\sigma^2}\right]$$

Importantly, the form of the normal MGF leads to
very fast decay in the tail. It is natural to wonder whether other random
variables exhibit similar rates. This motivates the
following definition.

\begin{definition} [sub-Gaussian random variable]
	Let $X$ be a mean-zero random variable taking values in $\R$. We say $X$ is sub-Gaussian with
	parameter $\sigma^2$ if for all $\lambda \in \R$:

	$$\ev{e^{\lambda X}} \leq e^{\lambda^2\sigma^2 / 2}$$

	We write $X \in \subG(\sigma^2)$.
\end{definition} 

Clearly, any sub-Gaussian random variable with achieve the same tail rate as the
corresponding normal.

\begin{definition}[sub-Gaussian random vector]
	Let $X$ be a mean zero random vector taking values in $\R^d$. We say $X$
	is $\sigma^2$ sub-Gaussian  if $X^Tu \in \subG(\sigma^2)$ for any unit
	vector $u \in \R^d$.
\end{definition}

Sub-gaussians have many useful properties. We will be particularly interested in the maximum (or $\infty$-norm) of
sub-Gaussian vectors. The key lemma is stated below (proof follows by a union bound).

\begin{lemma}\label{lemma:maximal}
	Let $X_1, ..., X_n \in \subG(\sigma^2)$. Then

	$$\max_{i = 1...n}|X_i| \lesssim \sigma\sqrt{\log n}$$

	with high probability.
\end{lemma}

\section{The LASSO}


Suppose a linear model

$$Y = X\theta^{*} + \eps$$

with $Y \in \R^n, \X \in R^{n \times d}, \theta^{*} \in \R^{d}$ and $\eps \in \R^{n}$. Furthermore, suppose that each element of $\eps$ are independent random variables, with a distribution to be specified later. Our goal is to estimate the vector $\theta^{*}$. A naive approach (which uses no further assumptions) is least squares, which will achieve a convergence rate of $\frac{d}{n}$. In the high-dimensional setting when $\frac{d}{n} \rightarrow \gamma > 0$, this approach is not good enough. Even as the sample size tends to infinity, the error will stay bounded away from zero. We get around this by placing further assumptions on $\theta^{*}$.

In particular, assume that $\theta^{*}$ is $s$-sparse, supported on an index set $S$. A natural estimator is the LASSO, which induces sparsity in the estimate by way of an $\ell_{1}$ penalty:

$$\hat{\theta} \in \argmin_{\theta \in \R^{d}}\left\{\frac{1}{2n}\norm{Y - X\theta}^{2}_{2} + \lambda \norm{\theta}_{1}\right\}$$
We now aim to study the convergence rate of the LASSO estimator, given our knowledge of the support of $\theta^{*}$. First, we need a couple of definitions. For any index set $S \subset [d]$ and $\alpha \geq 1$, define

$$\cC_{\alpha}(S) = \left\{ \Delta \in \R^{d} : \norm{\Delta_{S^{c}}}_{1} \leq \alpha \norm{\Delta_{S}}_{1}\right\}$$

The set defined abvoe constains the set of 'good' vectors with respect to $S$; i.e., those for which the $\ell_{1}$ norm on $S$ is greater than the $\ell_{1}$ norm off of $S$. Clearly, every vector supported exactly on $S$ lies in $\cC_{\alpha}(S)$. Importantly, if $\Delta \in \cC_{\alpha}(S)$, then

\begin{align*}
  \norm{\Delta}_{1}
  &= \norm{\Delta_{S}}_{1} + \norm{\Delta_{S^{c}}}_{1} \\
  &\leq (\alpha + 1)\norm{\Delta_{S}}_{1} \\
  &\leq (\alpha + 1)\sqrt{s}\norm{\Delta}_{2}
\end{align*}

where $s = |S|$. The first equality in the computation is a result of the decomposability of the $\ell_{1}$ norm over an index set $S$, and the final inequality follows by Cauchy-Schwarz.  The result above tells us that when $s << d$, inclusion in $\cC_{\alpha}(S)$ grants much tighter control over the $\ell_{1}$ norm than the usual bound $\norm{x}_{1} \leq \sqrt{d}\norm{x}_{2}$.

We say that the matrix $X$ satisfies the \textit{restricted eigenvalue} condition over $S$ with paramters $(k, \alpha)$ if

$$\frac1n\norm{X\Delta}_{2}^{2} \geq k\norm{\Delta}_{2}^{2} \quad \forall \Delta \in \cC_{\alpha}(S)$$

This condition is closely related to the curvature of the loss function. Observe that the Hessian of the loss function is $\frac{1}{2n} X^{T}X$: we want this matrix to be positive definite so that our optimization problem is strictly convex. However, this is impossible in the high-dimensional setting, so we only require positive eigenvalues in the 'good' directions, defined by the set $\cC_{\alpha}(S)$.

\begin{theorem}[LASSO Convergence]\label{thm:lasso-conv}
  Assume that $\theta^{*}$ has support $S \subset [d]$ with size $|S| = s$, and suppose that $X$ satisfies the restricted eigenvalue condition over $S$ with parameters $(k, 3)$. Then any LASSO solution $\hat{\theta}$ computed with tuning paramter $\lambda \geq 2\norm{\frac1n X^{T}\eps}_{\infty}$ satisfies

  $$\norm{\hat{\theta} - \theta^{*}}_{2} \lesssim \frac{\sqrt{s}}{k}\lambda$$
\end{theorem}

To prove Theorem \ref{thm:lasso-conv}, we state and prove two Lemmas that will yield the result.

\begin{lemma}[Deviation inequalities]\label{lemma:dev-ineq-lasso}
	For any $\theta^{*} \in \R^{d}$ supported on $S$ and any $\Delta \in \R^{d}$, we have
	$$\norm{\theta^{*} + \Delta}_{1} - \norm{\theta^{*}}_{1} \geq \norm{\Delta_{S^{c}}}_{1} - \norm{\Delta_{S}}_{1}$$
	Furthermore, if $\lambda \geq 2\norm{\frac1n X^{T}\eps}_{\infty}$, we have

	$$\frac{1}{2n}\norm{Y - X(\theta^{*} + \Delta)}_{2}^{2} - \frac{1}{2n}\norm{Y - X\theta^{*}}_{2}^{2} \geq -\frac{\lambda}{2}\left(\norm{\Delta_{S}}_{1} + \norm{\Delta_{S^{c}}}_{1}\right)$$
\end{lemma}

\begin{proof}
  To prove the first claim, observe

  \begin{align*}
	\norm{\theta^{*} + \Delta}_{1}
	&= \norm{\theta^{*}_{S} + \theta^{*}_{S^{c}} + \Delta_{S} + \Delta_{S^{c}}} \\
	&= \norm{\theta^{*}_{S} + \Delta_{S} + \Delta_{S^{c}}} \quad \textrm{(since $\theta^{*}$ is supported on $S$)} \\
	&\geq \norm{\theta^{*}_{S} + \Delta_{S^{c}}}_{1} - \norm{\Delta_{S}}_{1} \quad \textrm{(triangle inequality)} \\
	&= \norm{\theta^{*}_{S}}_{1} + \norm{\Delta_{S^{c}}}_{1} - \norm{\Delta_{S}}_{1} \quad \textrm{(decomposability)} \\
	&= \norm{\theta^{*}}_{1} + \norm{\Delta_{S^{c}}}_{1} - \norm{\Delta_{S}}_{1} \quad \textrm{(support of $\theta^{*}$)}
  \end{align*}

  Rearranging terms gives the result. To prove the second claim, we first introduce the notation

  $$\cL(\theta) = \frac{1}{2n}\norm{Y - X\theta}_{2}^{2}$$

  Observe that $\cL(.)$ is a convex function. Using this, we have

  \begin{align*}
	\cL(\theta^{*} + \Delta) - \cL(\theta^{*})
	&\geq \nabla \cL(\theta^{*})^{T}\Delta \quad \textrm{(first order condition for convex functions)} \\
	& \geq - |\nabla \cL(\theta^{*})^{T}\Delta | \\
	&\geq -\norm{\nabla \cL(\theta^{*})}_{\infty}\norm{\Delta}_{1} \quad \textrm{(Holder)} \\
	&= - \norm{\frac1nX^{T}\eps}_{\infty}(\norm{\Delta_{S}}_{1} + \norm{\Delta_{S^{c}}}_{1}) \\
	&\geq - \frac{\lambda}{2}(\norm{\Delta_{S}}_{1} + \norm{\Delta_{S^{c}}}_{1})
  \end{align*}

  as desired.
\end{proof}

\begin{lemma}[Control of error vector]\label{lemma:error-in-cone-lasso}
  Let $\hat{\theta}$ be the solution to the LASSO program with tuning parameter $\lambda \geq 2\norm{\frac1nX^{T}\eps}_{\infty}$. Then $\hat{\Delta} = \hat{\theta} - \theta^{*} \in \cC_{3}(S)$.
\end{lemma}

\begin{proof}
  Optimality of $\hat{\theta}$ tells us

  $$\frac{1}{2n}\norm{Y - X\hat{\theta}}_{2}^{2} + \lambda\norm{\hat{\theta}}_{1} \leq \frac{1}{2n}\norm{Y - X\theta^{*}}_{2}^{2} + \lambda\norm{\theta^{*}}_{1}$$

  Using $\hat{\theta} = \hat{\Delta} + \theta^{*}$ and rearranging terms gives

$$0 \geq \frac{1}{2n}\norm{Y - X(\theta^{*} + \hat{\Delta})}_{2}^{2} - \frac{1}{2n}\norm{Y - X\theta^{*}}_{2}^{2} + \lambda(\norm{\hat{\Delta} + \theta^{*}}_{1} - \norm{\theta^{*}}_{1})$$

Applying Lemma \ref{lemma:dev-ineq-lasso} to the terms in the above inequality gives

$$0 \geq \lambda\left(\norm{\hat{\Delta}_{S^{c}}}_{1} - \norm{\hat{\Delta}_{S}}_{1} - \frac12\norm{\hat{\Delta}_{S}}_{1} - \frac12\norm{\hat{\Delta}_{S^{c}}}_{1}\right)$$

Rearranging gives

$$3\norm{\hat{\Delta}_{S}}_{1} \geq \norm{\hat{\Delta}_{S^{c}}}$$

which shows $\hat{\Delta} \in \cC_{3}(S)$.

\end{proof}

Now we can prove Theorem \ref{thm:lasso-conv}.

\begin{proof}[Proof of the Theorem]

  We begin by formulating a \textit{basic inequality}. Optimality tells us

  $$\frac{1}{2n}\norm{Y - X\hat{\theta}}_{2}^{2} + \lambda\norm{\hat{\theta}}_{1} \leq \frac{1}{2n}\norm{Y - X\theta^{*}}_{2}^{2} + \lambda\norm{\theta^{*}}_{1}$$

  Additionally, observe

  $$\norm{Y - X\hat{\theta}}_{2}^{2} = \norm{X\theta^{*} + \eps - X\hat{\theta}}_{2}^{2} = \norm{X\theta^{*} - X\hat{\theta}}_{2}^{2} - 2(X(\hat{\theta} - \theta^{*}))^{T}\eps + \norm{\eps}_{2}^{2}$$

  Substituting this into the above inequality grants us the following computation.

  \begin{align*}
    \frac{1}{2n}\norm{X\hat{\Delta}}_{2}^{2}
    &\leq \frac1n\hat{\Delta}^{T}X^{T}\eps + \lambda (\norm{\theta^{*}}_{1} - \norm{\hat{\theta}}_{1}) \\
    &= \frac1n\hat{\Delta}^{T}X^{T}\eps - \lambda (\norm{\hat{\Delta} + \theta^{*}}_{1} - \norm{\theta^{*}}_{1}) \\
    &\leq \frac1n\hat{\Delta}^{T}X^{T}\eps + \lambda(\norm{\hat{\Delta_{S}}}_{1} - \norm{\hat{\Delta}_{S^{c}}}_{1}) \quad \textrm{(applying Lemma \ref{lemma:dev-ineq-lasso})} \\
    &\leq \norm{\frac1nX^{T}\eps}_{\infty}\norm{\hat{\Delta}}_{1} + \lambda\norm{\hat{\Delta}_{S}}_{1} \quad \textrm{(Holder's inequality)} \\
    &\leq \frac{\lambda}{2}\norm{\hat{\Delta}}_{1} + \lambda\norm{\hat{\Delta}_{S}}_{1} \\
    &\leq \frac32 \lambda \norm{\hat{\Delta}}_{1} \\
    &\leq 6\lambda\sqrt{s}\norm{\hat{\Delta}}_{2} \quad \textrm{(since $\hat{\Delta} \in \cC_{3}(S)$, see comment above)}
  \end{align*}

  Additionally, since $\hat{\Delta} \in \cC_{3}(S)$ and $X$ satisfies the restriced eigenvalue condition by assumption, we know that $\frac1n\norm{X\hat{\Delta}}_{2}^{2} \geq k\norm{\hat{\Delta}}^{2}_{2}$. This yields

  $$\frac{k}{2}\norm{\hat{\Delta}}_{2}^{2} \leq 6\lambda\sqrt{s}\norm{\hat{\Delta}}_{2}$$

  Rearranging terms gives 

  $$\norm{\hat{\Delta}}_2 \lesssim \frac{\sqrt{s}}{k}\lambda$$

  as desired.

\end{proof}

\subsection{Estimation under a statistical model}

Until now, the results we have given are entirely deterministic. Randomness
enters the equation when we suppose a model for our error terms. 

In particular,
suppose that the entries of $\eps$ are iid $\subG(\sigma^2)$ with zero mean.
Furthermore, we normalize the columns of $X$ so that they have $\ell_2$ norm at
most $\sqrt{n}$, and still assume that $X$ satisfies the restricted eigenvalue
condition. Under these conditions, $\frac1nX^T\eps$ is a sub-Gaussian vector
of dimension $d$ with parameter $\sigma^2/n$. Thus by Lemma \ref{lemma:maximal},
it follows 

$$\norm{\frac1nX^T\eps}_{\infty} \lesssim \sigma\sqrt{\frac{\log d}{n}}$$

with high probability. Since our results are conditional on $\lambda \geq
2\norm{\frac1nX^T\eps}_{\infty}$, take $\lambda = 2\sigma\sqrt{\log d / n}$, and
Theorem \ref{thm:lasso-conv} will hold with high probability. In particular,
this gives 

$$\norm{\hat{\Delta}}_2 \lesssim \frac{\sigma}{k}\sqrt{\frac{s\log d}{n}}$$
\end{document}
