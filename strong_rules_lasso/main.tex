\documentclass{amsart}

\newcommand{\R}{\mathbb{R}}
\newcommand{\dom}{\textrm{dom}}

\usepackage{amssymb, amsmath}
\usepackage{enumerate}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}

\title{Strong Rules for Efficient LASSO Computations and the BASIL Algorithm}
\author{Notes by Parker Knight}


\begin{document}

\maketitle

\section{Strong rules for the LASSO}

\subsection{Subgradients}

Let $f: \R^m \rightarrow \R$ be convex \cite{boyd_convex_2004}.

\bigskip

Recall the following first order condition: if $f$ is differentiable, then 

$$f(y) \geq f(x) + \nabla f(x)^T(y - x) \quad \forall x, y \in \dom(f)$$

What if $f$ is not differentiable? This motivates the following definition: Call
$g \in \R^m$ a \textit{subgradient} of $f$ at $x$ iff

$$f(y) \geq f(x) + g^T(y - x) \quad \forall x, y \in \dom(f)$$

The subdifferential of $f$ at $x$, denote $\partial f(x)$, is the set of all
subgradients. The following facts will be useful:

\begin{enumerate}
	\item If $f$ is differentiable at $x$, then $\partial f(x) = \{\nabla f(x) \}$
	\item For $\alpha_1, \alpha_2 \geq 0$, then $\partial \left[
	\alpha_1 f_1(x) + \alpha_2 f_2(x) \right] = \alpha_1 \partial f_1(x) +
	\alpha_2 \partial f_2(x)$
	\item $x^*$ minimizes $f$ iff $0 \in \partial f(x^*)$
\end{enumerate}

where we define set addition as $A + B = \{a + b | a \in A, b \in B \}$. 
\subsection{The LASSO}

Recall the LASSO \cite{tibshirani_strong_2012} loss function:

$$Q_{\lambda}(\beta) = \|Y - X\beta \|_2^2 + \lambda \|\beta \|_1$$

where the LASSO solution $\hat{\beta}(\lambda)$ satisfies

$$Q_\lambda(\hat{\beta}(\lambda)) = \min_{\beta}Q_{\lambda}(\beta)$$

Note that $Q_{\lambda}(.)$ is not differentiable everywhere, but it is convex.
So $\hat{\beta}_{\lambda}$ minimizes $Q$ iff $0 \in \partial
Q_{\lambda}(\hat{\beta}(\lambda))$. 

\bigskip

But how do we find $\partial Q$?

$$\partial Q_{\lambda}(b) = -X^T(Y - Xb) + \lambda \gamma$$

where $\gamma \in \partial ||b||_1$. For $g(x) = |x|$, we have 

$$\partial g (x) = \begin{cases}
	\{ 1 \} \quad x > 0 \\
	\{ -1 \} \quad x < 0 \\
	[-1,1] \quad x = 0
\end{cases}$$

since $g$ is differentiable when $x \neq 0$, and when $x = 0$ we have $|y| \geq
\alpha y$ iff $\alpha \in [-1,1]$. We extend this component-wise\footnote{Proof
is simple, but requires a bit more subgradient calculus to do formally.} to get the
expression for $\gamma$:

$$\gamma_j \in \begin{cases}
	\{ 1 \} \quad b_j > 0 \\
	\{ -1 \} \quad b_j < 0 \\
	[-1,1] \quad b_j = 0
\end{cases}$$

for $j = 1, ..., p$. Using this, we have that $\hat{\beta}(\lambda)$ minimizes
$Q$ iff 

$$X^T_j(Y - X\hat{\beta}(\lambda)) = \lambda \gamma_j$$

for $j = 1, ..., p$ with $\gamma_j$ defined as above for $\hat{\beta}(\lambda)$.
This admits a key detail: $|X^T_j(Y - X\hat{\beta}(\lambda))| < \lambda$ implies
$\hat{\beta}_j(\lambda) = 0$.



\subsection{Strong rules}

The material in this section comes largely from \cite{tibshirani_strong_2012}.

\bigskip

Suppose we have a grid of tuning parameters $\lambda_1, ..., \lambda_L$, which
which we seek exact LASSO solutions. The optimality condition described above
can lead us to an algorithm which will return exact solutions, but only
needs to solve a series of smaller sub-problems.

\bigskip

First we need an additional assumption. Let $c_j(\lambda_k) = X_j^T(Y -
X\hat{\beta}(\lambda_k))$ for $j = 1, ..., p$. 

\begin{assumption}\label{a1}
$|c_j(\lambda) - c_j(\tilde{\lambda})| \leq |\lambda - \tilde{\lambda}| \quad
\forall \lambda, \tilde{\lambda} > 0$
\end{assumption}

This says that $c_j(.)$ is non-expansive in its argument.

\begin{theorem}\label{t1}
	Suppose $|c_j(\lambda_{k-1})| < 2\lambda_k - \lambda_{k-1}$ and
	Assumption \ref{a1} holds. Then $\hat{\beta}_j(\lambda_k) = 0$. 
\end{theorem}

\begin{proof}
	Observe
	\begin{align*}
		|c_j(\lambda_k)| &= |c_j(\lambda_k) - c_j(\lambda{k-1}) + c_j(\lambda_{k-1})| \\
		&\leq |c_j(\lambda_k) - c_j(\lambda{k-1})| + |c_j(\lambda_{k-1})| \\
		&< (\lambda_{k-1} - \lambda_k) + (2\lambda_k - \lambda_{k-1}) \\
		&= \lambda_k
	\end{align*}	
	By the optimality condition for the LASSO problem this implies
	$\hat{\beta}_j(\lambda_k) = 0$. 
\end{proof}

Theorem \ref{t1} yields a natural algorithm for computing the LASSO solution for
a grid of tuning parameters.

\subsubsection{The strong rules algorithm}

This method requires data $X$, outcome $Y$, tuning parameters $\lambda_1 ...
\lambda_L$, and an initial estimate $\hat{\beta}(\lambda_1)$\footnote{This
initial estimate could correspond to an intercept-only model, for instance.}
Then, for $k = 1, ..., L-1$:

\begin{enumerate}[I]
	\item Let $\upsilon \subset \{1, ..., p\}$ denote the set of eligible predictors, and
	let $S(\lambda_k) \subset \{1,...,p\} = \{j : |c_j(\lambda_k)| >=
	2\lambda_{k+1} - \lambda_k \}$. Set $\upsilon = S(\lambda_k)$. 
	\item Solve the LASSO problem using only the predictors in $\upsilon$. 
	\item Check the subgradient optimality condition at \textit{all}
	predictors in $X$. If none of them violate the condition, we are done, yielding $\hat{\beta}(\lambda_{k+1})$. If any of them violate the condition, add these
	predictors to $\upsilon$ and repeat steps two and three.
\end{enumerate}


\section{BASIL}

The Batch Screening Iterative LASSO (BASIL) \cite{qian_fast_2020} algorithm takes advantage of the
strong rules method to yield a correct LASSO solution for a set of tuning
parameters while minimizing IO operations. This is valuable for massive genetic
data, where the number of predictors may be too large to load into memory, and
IO disk operations become expensive. 

\subsection{The Algorithm} The BASIL algorithm is comprised of three main
phases: screening (removing variables according to the strong rules), fitting
(compute the LASSO fit with a subset of the variables), and checking (ensuring
that optimality conditions are satisfied at all covariates).

\bigskip

The algorithm requires the following inputs:

\begin{itemize}
	\item data $X, Y, \Omega = \{1, ..., p \}$
	\item initial residual $r^{(0)}$ and active set $A^{(0)} = \emptyset$. 
	\item integers $M, \ell, \ell'$
	\item master tuning parameter list $\Lambda = \{\lambda_1, ...,
	\lambda_L \}$ and initial list $\Lambda^{(0)} = \{ \lambda_1, ...,
	\lambda_{\ell} \}$
\end{itemize}

Then, at iterate $k$:

\subsubsection{Screening} For each $j \in \Omega / A^{(k)}$, compute $c_j^{(k)} =
X_j^Tr^{(k)}$. Let $\upsilon_M^{(k)}$ denote the set of indices in $\Omega / A^{(k)}$
with the $M$ largest values of $c_j^{(k)}$. Define the strong set $S^{(k)} =
A^{(k)} \cup \upsilon_M^{(k)}$. The key assumption of this step: among the
variables that aren't already known to be active, those with the $M$ largest $c$
values are likely to be active (pass the strong rules screening) at this
iterate. This assumption is highly sensitive to the choice of $M$, see the
Discussion in \cite{qian_fast_2020} for more.


\subsubsection{Fitting} For each $\lambda \in \Lambda^{(k)}$ fit the LASSO using
only the covariates in $S^{(k)}$. Save $\hat{\beta}^{(k)}(\lambda)$ and residual
$r^{(k)}(\lambda)$.

\subsubsection{Checking} Find the smallest $\lambda \in \Lambda^{(k)}$ at which
the KKT optimality condition is satisfied:

$$\bar{\lambda}^{(k)} = \min \left\{\lambda \in \Lambda^{(k)} : \max_{j \in \Omega /
S^{(k)}} \left[\frac1n|X_j^Tr^{(k)}(\lambda)| \right] < \lambda \right\}$$

If this set is empty, increase $M$ by $\Delta M$ and repeat the Screening and
Fitting steps (this wil increase the size of the strong set). Otherwise, let
$r^{(k+1)} = r^{(k)}(\bar{\lambda}^{(k)})$, set $A^{(k+1)}$ to be the active set
at $\hat{\beta}^{(k)}(\bar{\lambda}^{(k)})$, and let $\Lambda^{(k+1)} = \{
\lambda \in \Lambda^{(k)} : \lambda < \bar{\lambda}^{(k)} \} \cup \{\textrm{the
next  } \ell' \textrm{  tuning parameters in  } \Lambda \}$. 

\bigskip

For $\lambda \in
\Lambda^{(k)} / \Lambda^{(k+1)}$, we found exact LASSO solutions. Continue
iterating until we have solutions for all $\lambda \in \Lambda$. 

\bibliography{refs}
\bibliographystyle{ieeetr}
\end{document}